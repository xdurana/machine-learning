{"name":"Machine Learning & Computational Neuroscience","tagline":"","body":"# Machine Learning & Computational Neuroscience\r\n\r\n## Courses\r\n\r\n### [Learning From Data](https://work.caltech.edu/telecourse.html)\r\n#### California Institute of Technology\r\nProf. [Yaser S. Abu-Mostafa](https://work.caltech.edu/)\r\n\r\nThis is an introductory course in machine learning (ML) that covers the basic theory, algorithms, and applications. ML is a key technology in Big Data, and in many financial, medical, commercial, and scientific applications. It enables computational systems to adaptively improve their performance with experience accumulated from the observed data. ML has become one of the hottest fields of study today, taken up by undergraduate and graduate students from 15 different majors at Caltech. This course balances theory and practice, and covers the mathematical as well as the heuristic aspects.\r\n\r\n---\r\n\r\n### [Machine learning](https://www.coursera.org/learn/machine-learning)\r\n#### Stanford University\r\nProf. [Andrew Ng](http://www.andrewng.org/)\r\n\r\nMachine learning is the science of getting computers to act without being explicitly programmed. In the past decade, machine learning has given us self-driving cars, practical speech recognition, effective web search, and a vastly improved understanding of the human genome. Machine learning is so pervasive today that you probably use it dozens of times a day without knowing it. Many researchers also think it is the best way to make progress towards human-level AI. In this class, you will learn about the most effective machine learning techniques, and gain practice implementing them and getting them to work for yourself. More importantly, you'll learn about not only the theoretical underpinnings of learning, but also gain the practical know-how needed to quickly and powerfully apply these techniques to new problems. Finally, you'll learn about some of Silicon Valley's best practices in innovation as it pertains to machine learning and AI.\r\n\r\n---\r\n\r\n### [Neural Networks for Machine Learning](https://www.coursera.org/course/neuralnets)\r\n#### University of Toronto\r\nProf. [Geoffrey Hinton](http://www.cs.toronto.edu/~hinton/)\r\n\r\nLearn about artificial neural networks and how they're being used for machine learning, as applied to speech and object recognition, image segmentation, modeling language and human motion, etc. We'll emphasize both the basic algorithms and the practical tricks needed to get them to work well.\r\n\r\n---\r\n\r\n### [Convolutional Neural Networks for Visual Recognition](http://cs231n.stanford.edu/)\r\n#### Stanford University\r\nProf. [Fei-Fei Li](http://vision.stanford.edu/index.html)\r\n\r\nComputer Vision has become ubiquitous in our society, with applications in search, image understanding, apps, mapping, medicine, drones, and self-driving cars. Core to many of these applications are visual recognition tasks such as image classification, localization and detection. Recent developments in neural network (aka “deep learning”) approaches have greatly advanced the performance of these state-of-the-art visual recognition systems. This course is a deep dive into details of the deep learning architectures with a focus on learning end-to-end models for these tasks, particularly image classification. During the 10-week course, students will learn to implement, train and debug their own neural networks and gain a detailed understanding of cutting-edge research in computer vision. The final assignment will involve training a multi-million parameter convolutional neural network and applying it on the largest image classification dataset (ImageNet). We will focus on teaching how to set up the problem of image recognition, the learning algorithms (e.g. backpropagation), practical engineering tricks for training and fine-tuning the networks and guide the students through hands-on assignments and a final course project. Much of the background and materials of this course will be drawn from the ImageNet Challenge.\r\n\r\n---\r\n\r\n### [Machine Learning](https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/)\r\n#### University of Oxford\r\nProf. [Nando de Freitas](http://www.cs.ox.ac.uk/people/nando.defreitas/)\r\n\r\nMachine learning techniques enable us to automatically extract features from data so as to solve predictive tasks, such as speech recognition, object recognition, machine translation, question-answering, anomaly detection, medical diagnosis and prognosis, automatic algorithm configuration, personalisation, robot control, time series forecasting, and much more. Learning systems adapt so that they can solve new tasks, related to previously encountered tasks, more efficiently.\r\n\r\nThe course focuses on the exciting field of deep learning. By drawing inspiration from neuroscience and statistics, it introduces the basic background on neural networks, back propagation, Boltzmann machines, autoencoders, convolutional neural networks and recurrent neural networks. It illustrates how deep learning is impacting our understanding of intelligence and contributing to the practical design of intelligent machines.\r\n\r\n---\r\n\r\n### [Data Science Specialization](https://www.coursera.org/specializations/jhudatascience)\r\n#### Johns Hopkins University\r\n\r\nThis Specialization covers the concepts and tools you'll need throughout the entire data science pipeline, from asking the right kinds of questions to making inferences and publishing results. In the final Capstone Project, you’ll apply the skills learned by building a data product using real-world data. At completion, students will have a portfolio demonstrating their mastery of the material.\r\n\r\n---\r\n\r\n### [Computational Neuroscience](https://www.coursera.org/course/compneuro)\r\n#### University of Washington\r\nProf. [Rajesh P. N. Rao](https://www.coursera.org/instructor/rajeshrao)\r\n\r\nThis course provides an introduction to basic computational methods for understanding what nervous systems do and for determining how they function. We will explore the computational principles governing various aspects of vision, sensory-motor control, learning, and memory. Specific topics that will be covered include representation of information by spiking neurons, processing of information in neural networks, and algorithms for adaptation and learning. We will make use of Matlab demonstrations and exercises to gain a deeper understanding of concepts and methods introduced in the course. The course is primarily aimed at third- or fourth-year undergraduates and beginning graduate students, as well as professionals and distance learners interested in learning how the brain processes information.\r\n\r\n## Books\r\n\r\n### Machine Learning\r\n\r\n- [The Elements of Statistical Learning](http://statweb.stanford.edu/~tibs/ElemStatLearn/)\r\n- [Bayesian Reasoning and Machine Learning](http://web4.cs.ucl.ac.uk/staff/D.Barber/pmwiki/pmwiki.php?n=Brml.HomePage)\r\n- [Machine Learning: a Probabilistic Perspective](http://www.cs.ubc.ca/~murphyk/MLbook/)\r\n- [Pattern Recognition and Machine Learning](http://research.microsoft.com/en-us/um/people/cmbishop/prml/)\r\n\r\n### Statistical Inference\r\n\r\n- [All of Statistics: A Concise Course in Statistical Inference](http://www.amazon.com/All-Statistics-Statistical-Inference-Springer/dp/0387402721)\r\n\r\n## Tools\r\n\r\n- [Python scikit-learn](http://scikit-learn.org/stable/)\r\n- [Data Science Ontology](http://www.datascienceontology.com/)\r\n- [Kaggle](https://www.kaggle.com/)\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}